---
layout: post
title: 通俗理解DNN深度神经网络
date: 2020-05-28
categories: AI
tags: AI 人工智能
---

本质上，CNN、DNN、RNN都是一种神经网络，包含输入、输出、隐含层，完成一个智能识别的过程，只是算法不一样而已。至于为什么要搞的这么复杂，而不是一个简单的一元二次函数那样，就比较难说清楚了。


## 人工神经网络的原理

受到生物神经元的启发，1943年心理学家McCulloch和数学家Pitts提出了人工神经元的概念，人工神经元又称为感知机，如下图所示。输入经过加权和偏置后，由激活函数处理后决定输出。



![](https://tva1.sinaimg.cn/large/007S8ZIlly1gf9219enrqj30fq095gn6.jpg)

其中生物神经元和人工神经元对应关系如下：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gf9222jj6aj309s03zdfr.jpg)

## 感知机
神经网络起源于20世纪五六十年代，那时候交感知机。拥有输入层、输出层、和一个隐含层。

## 多层感知机 MLP
（Multi-Layer perceptron,MLP）

顾名思义，多层感知机就是含有多个隐含层的神经网络。


![](https://tva1.sinaimg.cn/large/007S8ZIlly1gf8edamrl2j30o206xmxl.jpg)

## DNN

神经网络是基于感知机的扩展，DNN可以理解是多层隐含层的神经网络

## 优化函数
神经网络层数越来越多，图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid。

## 连续函数
sigmoid或tanh等连续函数模拟神经元对激励的响应

## 激活函数

是什么？将神经元的计算结果给一个输出，输出往往是二进制，1或0. 激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。激活函数需要满足一下几个条件：

- 1.非线性。如果激活函数是线性的，那么不管引入多少隐层，其效果和单层感知机没有任何差别。

- 2.可微性。训练网路时使用的基于梯度的优化方法需要激活函数必须可微。
（备注：可微函数是指那些在定义域中所有点都存在导数的函数）
- 3.单调性。单调性保证了神经网络模型简单。


对激活函数做扩展，感知机的激活函数是sign(z)虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gf8f16616hj306p02i3yb.jpg)

还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。以后会有专门的激活函数专栏。

## 长短时记忆单元LSTM

## 全连接

层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gf8eye4i6jj30nt0c3dzn.jpg)


所以，输出层上的神经元可以有不止一个输出


